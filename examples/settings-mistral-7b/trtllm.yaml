backend: tensorrtllm

source:
  type: huggingface
  id: mistralai/Mistral-7B-v0.1

tensorrtllm:
  convert_checkpoint_type: "llama"

  convert_checkpoint_args:
    - "--dtype=float16"
  trtllm_build_args:
    - "--gemm_plugin=float16"
    - "--gpt_attention_plugin=float16"
    - "--max_batch_size=128"
    - "--max_input_len=1024"
    - "--max_output_len=1024"
